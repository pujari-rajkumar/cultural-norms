{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "from transformers import *\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding computation for Chinese text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_emb(all_sents, tok_pooling='mean', get_cls_emb=False):\n",
    "    if len(all_sents) > 0:\n",
    "        with torch.cuda.device(0):\n",
    "            all_toks = emb_tokenizer.batch_encode_plus(all_sents, padding='longest',\\\n",
    "                                                   add_special_tokens=True)\n",
    "            tok_tensor = torch.tensor(all_toks['input_ids']).to('cuda')\n",
    "            tok_tensor = tok_tensor[:, :512]\n",
    "            with torch.no_grad():\n",
    "                model_out = emb_model(tok_tensor)\n",
    "                all_doc_tensor = model_out[0]\n",
    "                if get_cls_emb:\n",
    "                    all_doc_tensor = model_out[1]\n",
    "                all_doc_tensor = all_doc_tensor.to('cpu')\n",
    "            if get_cls_emb:\n",
    "                return all_doc_tensor\n",
    "            all_attn_mask = torch.tensor(all_toks['attention_mask'])\n",
    "            ret_tensor = torch.FloatTensor(all_doc_tensor.size(0), all_doc_tensor.size(-1))\n",
    "            for i in range(all_doc_tensor.size(0)):\n",
    "                slen = torch.sum(all_attn_mask[i, :])\n",
    "                if tok_pooling == 'mean':\n",
    "                    ret_tensor[i, :] = torch.mean(all_doc_tensor[i, :slen, :], dim=0)\n",
    "                elif tok_pooling == 'sum':\n",
    "                    ret_tensor[i, :] = torch.sum(all_doc_tensor[i, :slen, :], dim=0)\n",
    "                else:\n",
    "                    return 'invalid tok pooling'\n",
    "            return ret_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(all_sents, batch_size=100):\n",
    "    batches = []\n",
    "    beg = 0\n",
    "    end = batch_size\n",
    "    while beg < len(all_sents):\n",
    "        batches.append(all_sents[beg:end])\n",
    "        beg = end\n",
    "        end += batch_size\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_tokenizer_class = BertTokenizer\n",
    "emb_tokenizer = emb_tokenizer_class.from_pretrained('hfl/chinese-bert-wwm')\n",
    "with torch.cuda.device(0):\n",
    "    with torch.no_grad():\n",
    "        emb_model = BertModel.from_pretrained('hfl/chinese-bert-wwm',\\\n",
    "                                          output_hidden_states=False,\\\n",
    "                                          output_attentions=False)\n",
    "        emb_model.eval()\n",
    "        emb_model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/homes/rpujari/scratch1_fortytwo/DARPA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = json.load(open(dir_path + 'mpdd/metadata.json'))\n",
    "dialogue = json.load(open(dir_path + 'mpdd/dialogue.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25548 25548\n"
     ]
    }
   ],
   "source": [
    "all_utterances = []\n",
    "utterance_ids = []\n",
    "for conv_id in dialogue:\n",
    "    conv = dialogue[conv_id]\n",
    "    for i, turn in enumerate(conv):\n",
    "#         for key in turn:\n",
    "#             print(key, turn[key])\n",
    "        all_utterances.append(turn['utterance'])\n",
    "        utterance_ids.append(conv_id + '-' + str(i))\n",
    "print(len(utterance_ids), len(all_utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 256 done, 0:00:01.294949\n",
      "2 / 256 done, 0:00:01.825791\n",
      "3 / 256 done, 0:00:02.314426\n",
      "4 / 256 done, 0:00:02.979115\n",
      "5 / 256 done, 0:00:03.683817\n",
      "6 / 256 done, 0:00:04.163849\n",
      "7 / 256 done, 0:00:04.896527\n",
      "8 / 256 done, 0:00:05.318308\n",
      "9 / 256 done, 0:00:05.918438\n",
      "10 / 256 done, 0:00:06.448533\n",
      "11 / 256 done, 0:00:07.513016\n",
      "12 / 256 done, 0:00:08.073557\n",
      "13 / 256 done, 0:00:08.418241\n",
      "14 / 256 done, 0:00:08.922635\n",
      "15 / 256 done, 0:00:09.375076\n",
      "16 / 256 done, 0:00:09.661677\n",
      "17 / 256 done, 0:00:10.012492\n",
      "18 / 256 done, 0:00:10.358761\n",
      "19 / 256 done, 0:00:10.776029\n",
      "20 / 256 done, 0:00:11.408135\n",
      "21 / 256 done, 0:00:11.825479\n",
      "22 / 256 done, 0:00:12.188088\n",
      "23 / 256 done, 0:00:12.387972\n",
      "24 / 256 done, 0:00:12.680954\n",
      "25 / 256 done, 0:00:12.803513\n",
      "26 / 256 done, 0:00:12.975283\n",
      "27 / 256 done, 0:00:13.276735\n",
      "28 / 256 done, 0:00:13.735419\n",
      "29 / 256 done, 0:00:13.932247\n",
      "30 / 256 done, 0:00:14.192142\n",
      "31 / 256 done, 0:00:14.696530\n",
      "32 / 256 done, 0:00:14.946954\n",
      "33 / 256 done, 0:00:15.212822\n",
      "34 / 256 done, 0:00:15.436033\n",
      "35 / 256 done, 0:00:15.690972\n",
      "36 / 256 done, 0:00:15.888214\n",
      "37 / 256 done, 0:00:16.052116\n",
      "38 / 256 done, 0:00:16.336802\n",
      "39 / 256 done, 0:00:16.511846\n",
      "40 / 256 done, 0:00:16.767074\n",
      "41 / 256 done, 0:00:16.962988\n",
      "42 / 256 done, 0:00:17.111879\n",
      "43 / 256 done, 0:00:17.465751\n",
      "44 / 256 done, 0:00:17.603302\n",
      "45 / 256 done, 0:00:17.845873\n",
      "46 / 256 done, 0:00:17.975859\n",
      "47 / 256 done, 0:00:18.161790\n",
      "48 / 256 done, 0:00:18.338932\n",
      "49 / 256 done, 0:00:18.501946\n",
      "50 / 256 done, 0:00:18.664246\n",
      "51 / 256 done, 0:00:19.316444\n",
      "52 / 256 done, 0:00:19.544980\n",
      "53 / 256 done, 0:00:19.781795\n",
      "54 / 256 done, 0:00:19.978011\n",
      "55 / 256 done, 0:00:20.130596\n",
      "56 / 256 done, 0:00:20.308826\n",
      "57 / 256 done, 0:00:20.474078\n",
      "58 / 256 done, 0:00:20.660438\n",
      "59 / 256 done, 0:00:20.854024\n",
      "60 / 256 done, 0:00:21.151064\n",
      "61 / 256 done, 0:00:21.251944\n",
      "62 / 256 done, 0:00:21.445430\n",
      "63 / 256 done, 0:00:21.641890\n",
      "64 / 256 done, 0:00:21.799560\n",
      "65 / 256 done, 0:00:21.971095\n",
      "66 / 256 done, 0:00:22.208922\n",
      "67 / 256 done, 0:00:22.371059\n",
      "68 / 256 done, 0:00:22.550626\n",
      "69 / 256 done, 0:00:22.789045\n",
      "70 / 256 done, 0:00:22.929076\n",
      "71 / 256 done, 0:00:23.315522\n",
      "72 / 256 done, 0:00:23.504523\n",
      "73 / 256 done, 0:00:23.738954\n",
      "74 / 256 done, 0:00:23.916923\n",
      "75 / 256 done, 0:00:24.260474\n",
      "76 / 256 done, 0:00:24.385014\n",
      "77 / 256 done, 0:00:24.642928\n",
      "78 / 256 done, 0:00:24.870096\n",
      "79 / 256 done, 0:00:25.051473\n",
      "80 / 256 done, 0:00:25.204423\n",
      "81 / 256 done, 0:00:25.444365\n",
      "82 / 256 done, 0:00:25.676796\n",
      "83 / 256 done, 0:00:26.029307\n",
      "84 / 256 done, 0:00:26.257781\n",
      "85 / 256 done, 0:00:26.642911\n",
      "86 / 256 done, 0:00:26.946202\n",
      "87 / 256 done, 0:00:27.257353\n",
      "88 / 256 done, 0:00:28.004039\n",
      "89 / 256 done, 0:00:28.627426\n",
      "90 / 256 done, 0:00:29.140613\n",
      "91 / 256 done, 0:00:29.480884\n",
      "92 / 256 done, 0:00:29.787819\n",
      "93 / 256 done, 0:00:30.211507\n",
      "94 / 256 done, 0:00:30.575748\n",
      "95 / 256 done, 0:00:31.011845\n",
      "96 / 256 done, 0:00:31.520933\n",
      "97 / 256 done, 0:00:31.961998\n",
      "98 / 256 done, 0:00:32.508178\n",
      "99 / 256 done, 0:00:32.798882\n",
      "100 / 256 done, 0:00:33.070388\n",
      "101 / 256 done, 0:00:33.504775\n",
      "102 / 256 done, 0:00:33.920276\n",
      "103 / 256 done, 0:00:34.404570\n",
      "104 / 256 done, 0:00:34.700005\n",
      "105 / 256 done, 0:00:35.060878\n",
      "106 / 256 done, 0:00:35.348536\n",
      "107 / 256 done, 0:00:35.580132\n",
      "108 / 256 done, 0:00:35.846383\n",
      "109 / 256 done, 0:00:36.179490\n",
      "110 / 256 done, 0:00:36.474696\n",
      "111 / 256 done, 0:00:36.802661\n",
      "112 / 256 done, 0:00:37.194276\n",
      "113 / 256 done, 0:00:37.527947\n",
      "114 / 256 done, 0:00:37.872744\n",
      "115 / 256 done, 0:00:38.303945\n",
      "116 / 256 done, 0:00:38.677713\n",
      "117 / 256 done, 0:00:39.010559\n",
      "118 / 256 done, 0:00:39.371569\n",
      "119 / 256 done, 0:00:39.679148\n",
      "120 / 256 done, 0:00:40.071420\n",
      "121 / 256 done, 0:00:40.434349\n",
      "122 / 256 done, 0:00:40.759932\n",
      "123 / 256 done, 0:00:41.134210\n",
      "124 / 256 done, 0:00:41.451762\n",
      "125 / 256 done, 0:00:41.828335\n",
      "126 / 256 done, 0:00:42.228014\n",
      "127 / 256 done, 0:00:42.567865\n",
      "128 / 256 done, 0:00:43.583093\n",
      "129 / 256 done, 0:00:44.300045\n",
      "130 / 256 done, 0:00:44.977917\n",
      "131 / 256 done, 0:00:45.393272\n",
      "132 / 256 done, 0:00:45.714822\n",
      "133 / 256 done, 0:00:46.326174\n",
      "134 / 256 done, 0:00:46.950672\n",
      "135 / 256 done, 0:00:47.335221\n",
      "136 / 256 done, 0:00:48.164519\n",
      "137 / 256 done, 0:00:48.776724\n",
      "138 / 256 done, 0:00:49.084233\n",
      "139 / 256 done, 0:00:49.651347\n",
      "140 / 256 done, 0:00:50.415679\n",
      "141 / 256 done, 0:00:50.899393\n",
      "142 / 256 done, 0:00:51.429409\n",
      "143 / 256 done, 0:00:52.166299\n",
      "144 / 256 done, 0:00:52.500959\n",
      "145 / 256 done, 0:00:52.872743\n",
      "146 / 256 done, 0:00:53.627512\n",
      "147 / 256 done, 0:00:53.875525\n",
      "148 / 256 done, 0:00:54.165240\n",
      "149 / 256 done, 0:00:54.637920\n",
      "150 / 256 done, 0:00:54.924779\n",
      "151 / 256 done, 0:00:55.220629\n",
      "152 / 256 done, 0:00:55.506129\n",
      "153 / 256 done, 0:00:55.934873\n",
      "154 / 256 done, 0:00:56.162887\n",
      "155 / 256 done, 0:00:56.329718\n",
      "156 / 256 done, 0:00:56.512742\n",
      "157 / 256 done, 0:00:57.168150\n",
      "158 / 256 done, 0:00:57.326066\n",
      "159 / 256 done, 0:00:57.641161\n",
      "160 / 256 done, 0:00:57.988001\n",
      "161 / 256 done, 0:00:58.232975\n",
      "162 / 256 done, 0:00:58.598453\n",
      "163 / 256 done, 0:00:58.967549\n",
      "164 / 256 done, 0:00:59.109684\n",
      "165 / 256 done, 0:00:59.560091\n",
      "166 / 256 done, 0:00:59.846074\n",
      "167 / 256 done, 0:01:00.082142\n",
      "168 / 256 done, 0:01:00.320902\n",
      "169 / 256 done, 0:01:00.511134\n",
      "170 / 256 done, 0:01:00.773252\n",
      "171 / 256 done, 0:01:00.936121\n",
      "172 / 256 done, 0:01:01.309404\n",
      "173 / 256 done, 0:01:01.559225\n",
      "174 / 256 done, 0:01:01.801725\n",
      "175 / 256 done, 0:01:02.564896\n",
      "176 / 256 done, 0:01:02.793588\n",
      "177 / 256 done, 0:01:02.977809\n",
      "178 / 256 done, 0:01:03.194034\n",
      "179 / 256 done, 0:01:03.514202\n",
      "180 / 256 done, 0:01:03.777611\n",
      "181 / 256 done, 0:01:04.130848\n",
      "182 / 256 done, 0:01:04.308560\n",
      "183 / 256 done, 0:01:04.665140\n",
      "184 / 256 done, 0:01:04.849096\n",
      "185 / 256 done, 0:01:04.984698\n",
      "186 / 256 done, 0:01:05.300973\n",
      "187 / 256 done, 0:01:05.544006\n",
      "188 / 256 done, 0:01:05.838490\n",
      "189 / 256 done, 0:01:06.150771\n",
      "190 / 256 done, 0:01:06.355694\n",
      "191 / 256 done, 0:01:06.624308\n",
      "192 / 256 done, 0:01:06.881810\n",
      "193 / 256 done, 0:01:07.078668\n",
      "194 / 256 done, 0:01:07.249837\n",
      "195 / 256 done, 0:01:07.517764\n",
      "196 / 256 done, 0:01:07.751217\n",
      "197 / 256 done, 0:01:07.886379\n",
      "198 / 256 done, 0:01:08.274641\n",
      "199 / 256 done, 0:01:08.684413\n",
      "200 / 256 done, 0:01:08.940784\n",
      "201 / 256 done, 0:01:09.410122\n",
      "202 / 256 done, 0:01:10.461327\n",
      "203 / 256 done, 0:01:11.659279\n",
      "204 / 256 done, 0:01:12.375083\n",
      "205 / 256 done, 0:01:12.557512\n",
      "206 / 256 done, 0:01:13.521170\n",
      "207 / 256 done, 0:01:14.198450\n",
      "208 / 256 done, 0:01:14.471783\n",
      "209 / 256 done, 0:01:14.869715\n",
      "210 / 256 done, 0:01:15.311072\n",
      "211 / 256 done, 0:01:15.762158\n",
      "212 / 256 done, 0:01:16.291076\n",
      "213 / 256 done, 0:01:16.636510\n",
      "214 / 256 done, 0:01:17.082424\n",
      "215 / 256 done, 0:01:17.337121\n",
      "216 / 256 done, 0:01:17.847398\n",
      "217 / 256 done, 0:01:18.566351\n",
      "218 / 256 done, 0:01:18.966519\n",
      "219 / 256 done, 0:01:19.569947\n",
      "220 / 256 done, 0:01:19.734116\n",
      "221 / 256 done, 0:01:19.949943\n",
      "222 / 256 done, 0:01:20.054610\n",
      "223 / 256 done, 0:01:20.251004\n",
      "224 / 256 done, 0:01:20.443415\n",
      "225 / 256 done, 0:01:20.932766\n",
      "226 / 256 done, 0:01:21.167357\n",
      "227 / 256 done, 0:01:21.347699\n",
      "228 / 256 done, 0:01:22.206332\n",
      "229 / 256 done, 0:01:22.433996\n",
      "230 / 256 done, 0:01:23.101895\n",
      "231 / 256 done, 0:01:23.358612\n",
      "232 / 256 done, 0:01:23.816581\n",
      "233 / 256 done, 0:01:24.204674\n",
      "234 / 256 done, 0:01:24.626575\n",
      "235 / 256 done, 0:01:25.301149\n",
      "236 / 256 done, 0:01:25.577686\n",
      "237 / 256 done, 0:01:26.108426\n",
      "238 / 256 done, 0:01:26.866285\n",
      "239 / 256 done, 0:01:27.164693\n",
      "240 / 256 done, 0:01:27.597963\n",
      "241 / 256 done, 0:01:28.132753\n",
      "242 / 256 done, 0:01:28.341304\n",
      "243 / 256 done, 0:01:28.739454\n",
      "244 / 256 done, 0:01:29.153447\n",
      "245 / 256 done, 0:01:29.847276\n",
      "246 / 256 done, 0:01:30.408142\n",
      "247 / 256 done, 0:01:30.895531\n",
      "248 / 256 done, 0:01:31.194921\n",
      "249 / 256 done, 0:01:31.603870\n",
      "250 / 256 done, 0:01:32.068386\n",
      "251 / 256 done, 0:01:32.598785\n",
      "252 / 256 done, 0:01:32.918599\n",
      "253 / 256 done, 0:01:33.284924\n",
      "254 / 256 done, 0:01:33.739398\n",
      "255 / 256 done, 0:01:34.394290\n",
      "256 / 256 done, 0:01:34.681790\n"
     ]
    }
   ],
   "source": [
    "utterance_batches = batchify(all_utterances)\n",
    "batch_utterance_embs = []\n",
    "t1 = datetime.now()\n",
    "for i, batch in enumerate(utterance_batches):\n",
    "    batch_emb = create_bert_emb(batch)\n",
    "    batch_utterance_embs.append(batch_emb)\n",
    "    t2 = datetime.now()\n",
    "    print(i + 1, '/', len(utterance_batches), 'done,', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25548, 768])\n"
     ]
    }
   ],
   "source": [
    "utterance_embs = torch.cat(batch_utterance_embs, dim=0)\n",
    "print(utterance_embs.size())\n",
    "with open(dir_path + 'mpdd/bert-base-utterance-embs.pkl', 'wb') as outfile:\n",
    "    pickle.dump((utterance_embs, utterance_ids), outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating to English\n",
    "#### Argostranslate: https://www.argosopentech.com/argospm/index/\n",
    "#### huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = '那個憨女人有什麼值得送的，正鵬這個人也真是的！'\n",
    "sent2 = '哎喲，老婆子，你怎麼盡講那些不利於團結的話呢！他去送送他的同學也在情理之中嘛！'\n",
    "sent3 = '爸、媽，我回來啦！'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argostranslate import package, translate\n",
    "package.install_from_path(dir_path + 'translate-zh_en-1_1.argosmodel')\n",
    "installed_languages = translate.get_installed_languages()\n",
    "print([str(lang) for lang in installed_languages])\n",
    "translation_zh_en = installed_languages[1].get_translation(installed_languages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original translations given in the paper\n",
    "\n",
    "##### What is Zheng-Peng thinking? He has no need to send the silly woman home.\n",
    "##### Hey. My old woman. How can you say such uncoordinated words? It’s reasonable for him to send his classmate home.”\n",
    "##### Dad, Mom, I'm back!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google translate webpage (API is billed and should be used via Google cloud platform)\n",
    "\n",
    "##### What is there for that silly woman to give, and Zhengpeng is the real one!\n",
    "##### Alas, old lady, how can you say all those things that are not good for unity! It makes sense for him to send off his classmates!\n",
    "##### Dad, Mom, I'm back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was true that the female stereotyped had suffice, and that the perpetrators were.\n",
      "Alexandre, How you can impose a boycott that is negative. He was sent to his fellows.\n",
      "raz, I return!\n"
     ]
    }
   ],
   "source": [
    "#Translations using argos-translate offline model\n",
    "print(translation_zh_en.translate(sent1))\n",
    "print(translation_zh_en.translate(sent2))\n",
    "print(translation_zh_en.translate(sent3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translations using hugging-face translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_hf(chinese_sent):\n",
    "    batch = tokenizer([chinese_sent], return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(**batch)\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's so good about that bitch, Jung-ho?\n",
      "Honey, why are you saying things that are not good for unity? It's also reasonable for him to send his classmates!\n",
      "Mom and Dad, I'm home!\n"
     ]
    }
   ],
   "source": [
    "print(translate_hf(sent1))\n",
    "print(translate_hf(sent2))\n",
    "print(translate_hf(sent3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing into sentences (Stanza: https://stanfordnlp.github.io/stanza/tokenize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 27.0MB/s]                    \n",
      "INFO:stanza:Downloading default packages for language: zh-hant (Traditional_Chinese)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/zh-hant/default.zip: 100%|██████████| 236M/236M [00:41<00:00, 5.74MB/s] \n",
      "INFO:stanza:Finished downloading models and saved to /homes/rpujari/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('zh-hant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Loading these models for language: zh-hant (Traditional_Chinese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "=======================\n",
      "\n",
      "INFO:stanza:Use device: gpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: 哎喲\n",
      "id: (2,)\ttext: ，\n",
      "id: (3,)\ttext: 老\n",
      "id: (4,)\ttext: 婆子\n",
      "id: (5,)\ttext: ，\n",
      "id: (6,)\ttext: 你\n",
      "id: (7,)\ttext: 怎麼\n",
      "id: (8,)\ttext: 盡講\n",
      "id: (9,)\ttext: 那些\n",
      "id: (10,)\ttext: 不利\n",
      "id: (11,)\ttext: 於\n",
      "id: (12,)\ttext: 團結\n",
      "id: (13,)\ttext: 的\n",
      "id: (14,)\ttext: 話\n",
      "id: (15,)\ttext: 呢\n",
      "id: (16,)\ttext: ！\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: 他\n",
      "id: (2,)\ttext: 去\n",
      "id: (3,)\ttext: 送送\n",
      "id: (4,)\ttext: 他\n",
      "id: (5,)\ttext: 的\n",
      "id: (6,)\ttext: 同學\n",
      "id: (7,)\ttext: 也\n",
      "id: (8,)\ttext: 在\n",
      "id: (9,)\ttext: 情理\n",
      "id: (10,)\ttext: 之\n",
      "id: (11,)\ttext: 中嘛\n",
      "id: (12,)\ttext: ！\n"
     ]
    }
   ],
   "source": [
    "stanza_nlp = stanza.Pipeline(lang='zh-hant', processors='tokenize')\n",
    "doc = stanza_nlp(sent2)\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
