{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding computation for Chinese text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_emb(all_sents, tok_pooling='mean', get_cls_emb=False):\n",
    "    if len(all_sents) > 0:\n",
    "        with torch.cuda.device(0):\n",
    "            all_toks = emb_tokenizer.batch_encode_plus(all_sents, padding='longest',\\\n",
    "                                                   add_special_tokens=True)\n",
    "            tok_tensor = torch.tensor(all_toks['input_ids']).to('cuda')\n",
    "            tok_tensor = tok_tensor[:, :512]\n",
    "            with torch.no_grad():\n",
    "                model_out = emb_model(tok_tensor)\n",
    "                all_doc_tensor = model_out[0]\n",
    "                if get_cls_emb:\n",
    "                    all_doc_tensor = model_out[1]\n",
    "                all_doc_tensor = all_doc_tensor.to('cpu')\n",
    "            if get_cls_emb:\n",
    "                return all_doc_tensor\n",
    "            all_attn_mask = torch.tensor(all_toks['attention_mask'])\n",
    "            ret_tensor = torch.FloatTensor(all_doc_tensor.size(0), all_doc_tensor.size(-1))\n",
    "            for i in range(all_doc_tensor.size(0)):\n",
    "                slen = torch.sum(all_attn_mask[i, :])\n",
    "                if tok_pooling == 'mean':\n",
    "                    ret_tensor[i, :] = torch.mean(all_doc_tensor[i, :slen, :], dim=0)\n",
    "                elif tok_pooling == 'sum':\n",
    "                    ret_tensor[i, :] = torch.sum(all_doc_tensor[i, :slen, :], dim=0)\n",
    "                else:\n",
    "                    return 'invalid tok pooling'\n",
    "            return ret_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(all_sents, batch_size=100):\n",
    "    batches = []\n",
    "    beg = 0\n",
    "    end = batch_size\n",
    "    while beg < len(all_sents):\n",
    "        batches.append(all_sents[beg:end])\n",
    "        beg = end\n",
    "        end += batch_size\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "emb_tokenizer_class = BertTokenizer\n",
    "emb_tokenizer = emb_tokenizer_class.from_pretrained('hfl/chinese-bert-wwm')\n",
    "with torch.cuda.device(0):\n",
    "    with torch.no_grad():\n",
    "        emb_model = BertModel.from_pretrained('hfl/chinese-bert-wwm',\\\n",
    "                                          output_hidden_states=False,\\\n",
    "                                          output_attentions=False)\n",
    "        emb_model.eval()\n",
    "        emb_model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/homes/rpujari/scratch1_fortytwo/DARPA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = json.load(open(dir_path + 'mpdd/metadata.json'))\n",
    "dialogue = json.load(open(dir_path + 'mpdd/dialogue.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25548 25548\n"
     ]
    }
   ],
   "source": [
    "all_utterances = []\n",
    "utterance_ids = []\n",
    "for conv_id in dialogue:\n",
    "    conv = dialogue[conv_id]\n",
    "    for i, turn in enumerate(conv):\n",
    "#         for key in turn:\n",
    "#             print(key, turn[key])\n",
    "        all_utterances.append(turn['utterance'])\n",
    "        utterance_ids.append(conv_id + '-' + str(i))\n",
    "print(len(utterance_ids), len(all_utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utterance_batches = batchify(all_utterances)\n",
    "batch_utterance_embs = []\n",
    "t1 = datetime.now()\n",
    "for i, batch in enumerate(utterance_batches):\n",
    "    batch_emb = create_bert_emb(batch)\n",
    "    batch_utterance_embs.append(batch_emb)\n",
    "    t2 = datetime.now()\n",
    "    print(i + 1, '/', len(utterance_batches), 'done,', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_embs = torch.cat(batch_utterance_embs, dim=0)\n",
    "print(utterance_embs.size())\n",
    "with open(dir_path + 'mpdd/bert-base-utterance-embs.pkl', 'wb') as outfile:\n",
    "    pickle.dump((utterance_embs, utterance_ids), outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating to English\n",
    "#### Argostranslate: https://www.argosopentech.com/argospm/index/\n",
    "#### huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = '那個憨女人有什麼值得送的，正鵬這個人也真是的！'\n",
    "sent2 = '哎喲，老婆子，你怎麼盡講那些不利於團結的話呢！他去送送他的同學也在情理之中嘛！'\n",
    "sent3 = '爸、媽，我回來啦！'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English', 'Chinese']\n"
     ]
    }
   ],
   "source": [
    "from argostranslate import package, translate\n",
    "package.install_from_path(dir_path + 'translate-zh_en-1_1.argosmodel')\n",
    "installed_languages = translate.get_installed_languages()\n",
    "print([str(lang) for lang in installed_languages])\n",
    "translation_zh_en = installed_languages[1].get_translation(installed_languages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original translations given in the paper\n",
    "\n",
    "##### What is Zheng-Peng thinking? He has no need to send the silly woman home.\n",
    "##### Hey. My old woman. How can you say such uncoordinated words? It’s reasonable for him to send his classmate home.”\n",
    "##### Dad, Mom, I'm back!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google translate webpage (API is billed and should be used via Google cloud platform)\n",
    "\n",
    "##### What is there for that silly woman to give, and Zhengpeng is the real one!\n",
    "##### Alas, old lady, how can you say all those things that are not good for unity! It makes sense for him to send off his classmates!\n",
    "##### Dad, Mom, I'm back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was true that the female stereotyped had suffice, and that the perpetrators were.\n",
      "Alexandre, How you can impose a boycott that is negative. He was sent to his fellows.\n",
      "raz, I return!\n"
     ]
    }
   ],
   "source": [
    "#Translations using argos-translate offline model\n",
    "print(translation_zh_en.translate(sent1))\n",
    "print(translation_zh_en.translate(sent2))\n",
    "print(translation_zh_en.translate(sent3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translations using hugging-face translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_hf(chinese_sent):\n",
    "    batch = tokenizer([chinese_sent], return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(**batch)\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate_hf(sent1))\n",
    "print(translate_hf(sent2))\n",
    "print(translate_hf(sent3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing into sentences (Stanza: https://stanfordnlp.github.io/stanza/tokenize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('zh-hant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_nlp = stanza.Pipeline(lang='zh-hant', processors='tokenize')\n",
    "doc = stanza_nlp(sent2)\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
